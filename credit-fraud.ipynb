{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":23498,"sourceType":"datasetVersion","datasetId":310}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imported Libraries\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.patches as mpatches\nimport time\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport collections\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndf = pd.read_csv('../input/creditcard.csv')\ndf.head()","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:10:52.540075Z","iopub.execute_input":"2025-07-03T10:10:52.540681Z","iopub.status.idle":"2025-07-03T10:10:56.807012Z","shell.execute_reply.started":"2025-07-03T10:10:52.540627Z","shell.execute_reply":"2025-07-03T10:10:56.805890Z"},"_kg_hide-input":false},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   Time        V1        V2        V3        V4        V5        V6        V7  \\\n0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n\n         V8        V9  ...       V21       V22       V23       V24       V25  \\\n0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n\n        V26       V27       V28  Amount  Class  \n0 -0.189115  0.133558 -0.021053  149.62      0  \n1  0.125895 -0.008983  0.014724    2.69      0  \n2 -0.139097 -0.055353 -0.059752  378.66      0  \n3 -0.221929  0.062723  0.061458  123.50      0  \n4  0.502292  0.219422  0.215153   69.99      0  \n\n[5 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 31 columns</p>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:10:59.091847Z","iopub.execute_input":"2025-07-03T10:10:59.092268Z","iopub.status.idle":"2025-07-03T10:10:59.569238Z","shell.execute_reply.started":"2025-07-03T10:10:59.092239Z","shell.execute_reply":"2025-07-03T10:10:59.568262Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                Time            V1            V2            V3            V4  \\\ncount  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \nmean    94813.859575  1.175161e-15  3.369007e-16 -1.379537e-15  2.094852e-15   \nstd     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \nmin         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \nmax    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n\n                 V5            V6            V7            V8            V9  \\\ncount  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \nmean   1.021879e-15  1.500885e-15 -5.620335e-16  1.149614e-16 -2.426963e-15   \nstd    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \nmin   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \nmax    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n\n       ...           V21           V22           V23           V24  \\\ncount  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \nmean   ...  1.596686e-16 -3.576577e-16  2.650499e-16  4.472317e-15   \nstd    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \nmin    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \nmax    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n\n                V25           V26           V27           V28         Amount  \\\ncount  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \nmean   5.109395e-16  1.686100e-15 -3.662399e-16 -1.225457e-16      88.349619   \nstd    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \nmin   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \nmax    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n\n               Class  \ncount  284807.000000  \nmean        0.001727  \nstd         0.041527  \nmin         0.000000  \n25%         0.000000  \n50%         0.000000  \n75%         0.000000  \nmax         1.000000  \n\n[8 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>284807.000000</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>...</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>284807.000000</td>\n      <td>284807.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>94813.859575</td>\n      <td>1.175161e-15</td>\n      <td>3.369007e-16</td>\n      <td>-1.379537e-15</td>\n      <td>2.094852e-15</td>\n      <td>1.021879e-15</td>\n      <td>1.500885e-15</td>\n      <td>-5.620335e-16</td>\n      <td>1.149614e-16</td>\n      <td>-2.426963e-15</td>\n      <td>...</td>\n      <td>1.596686e-16</td>\n      <td>-3.576577e-16</td>\n      <td>2.650499e-16</td>\n      <td>4.472317e-15</td>\n      <td>5.109395e-16</td>\n      <td>1.686100e-15</td>\n      <td>-3.662399e-16</td>\n      <td>-1.225457e-16</td>\n      <td>88.349619</td>\n      <td>0.001727</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>47488.145955</td>\n      <td>1.958696e+00</td>\n      <td>1.651309e+00</td>\n      <td>1.516255e+00</td>\n      <td>1.415869e+00</td>\n      <td>1.380247e+00</td>\n      <td>1.332271e+00</td>\n      <td>1.237094e+00</td>\n      <td>1.194353e+00</td>\n      <td>1.098632e+00</td>\n      <td>...</td>\n      <td>7.345240e-01</td>\n      <td>7.257016e-01</td>\n      <td>6.244603e-01</td>\n      <td>6.056471e-01</td>\n      <td>5.212781e-01</td>\n      <td>4.822270e-01</td>\n      <td>4.036325e-01</td>\n      <td>3.300833e-01</td>\n      <td>250.120109</td>\n      <td>0.041527</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>-5.640751e+01</td>\n      <td>-7.271573e+01</td>\n      <td>-4.832559e+01</td>\n      <td>-5.683171e+00</td>\n      <td>-1.137433e+02</td>\n      <td>-2.616051e+01</td>\n      <td>-4.355724e+01</td>\n      <td>-7.321672e+01</td>\n      <td>-1.343407e+01</td>\n      <td>...</td>\n      <td>-3.483038e+01</td>\n      <td>-1.093314e+01</td>\n      <td>-4.480774e+01</td>\n      <td>-2.836627e+00</td>\n      <td>-1.029540e+01</td>\n      <td>-2.604551e+00</td>\n      <td>-2.256568e+01</td>\n      <td>-1.543008e+01</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>54201.500000</td>\n      <td>-9.203734e-01</td>\n      <td>-5.985499e-01</td>\n      <td>-8.903648e-01</td>\n      <td>-8.486401e-01</td>\n      <td>-6.915971e-01</td>\n      <td>-7.682956e-01</td>\n      <td>-5.540759e-01</td>\n      <td>-2.086297e-01</td>\n      <td>-6.430976e-01</td>\n      <td>...</td>\n      <td>-2.283949e-01</td>\n      <td>-5.423504e-01</td>\n      <td>-1.618463e-01</td>\n      <td>-3.545861e-01</td>\n      <td>-3.171451e-01</td>\n      <td>-3.269839e-01</td>\n      <td>-7.083953e-02</td>\n      <td>-5.295979e-02</td>\n      <td>5.600000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>84692.000000</td>\n      <td>1.810880e-02</td>\n      <td>6.548556e-02</td>\n      <td>1.798463e-01</td>\n      <td>-1.984653e-02</td>\n      <td>-5.433583e-02</td>\n      <td>-2.741871e-01</td>\n      <td>4.010308e-02</td>\n      <td>2.235804e-02</td>\n      <td>-5.142873e-02</td>\n      <td>...</td>\n      <td>-2.945017e-02</td>\n      <td>6.781943e-03</td>\n      <td>-1.119293e-02</td>\n      <td>4.097606e-02</td>\n      <td>1.659350e-02</td>\n      <td>-5.213911e-02</td>\n      <td>1.342146e-03</td>\n      <td>1.124383e-02</td>\n      <td>22.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>139320.500000</td>\n      <td>1.315642e+00</td>\n      <td>8.037239e-01</td>\n      <td>1.027196e+00</td>\n      <td>7.433413e-01</td>\n      <td>6.119264e-01</td>\n      <td>3.985649e-01</td>\n      <td>5.704361e-01</td>\n      <td>3.273459e-01</td>\n      <td>5.971390e-01</td>\n      <td>...</td>\n      <td>1.863772e-01</td>\n      <td>5.285536e-01</td>\n      <td>1.476421e-01</td>\n      <td>4.395266e-01</td>\n      <td>3.507156e-01</td>\n      <td>2.409522e-01</td>\n      <td>9.104512e-02</td>\n      <td>7.827995e-02</td>\n      <td>77.165000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>172792.000000</td>\n      <td>2.454930e+00</td>\n      <td>2.205773e+01</td>\n      <td>9.382558e+00</td>\n      <td>1.687534e+01</td>\n      <td>3.480167e+01</td>\n      <td>7.330163e+01</td>\n      <td>1.205895e+02</td>\n      <td>2.000721e+01</td>\n      <td>1.559499e+01</td>\n      <td>...</td>\n      <td>2.720284e+01</td>\n      <td>1.050309e+01</td>\n      <td>2.252841e+01</td>\n      <td>4.584549e+00</td>\n      <td>7.519589e+00</td>\n      <td>3.517346e+00</td>\n      <td>3.161220e+01</td>\n      <td>3.384781e+01</td>\n      <td>25691.160000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows Ã— 31 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Good No Null Values!\ndf.isnull().sum().max()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:11:01.460218Z","iopub.execute_input":"2025-07-03T10:11:01.460539Z","iopub.status.idle":"2025-07-03T10:11:01.499475Z","shell.execute_reply.started":"2025-07-03T10:11:01.460514Z","shell.execute_reply":"2025-07-03T10:11:01.498487Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:11:04.051219Z","iopub.execute_input":"2025-07-03T10:11:04.051544Z","iopub.status.idle":"2025-07-03T10:11:04.059531Z","shell.execute_reply.started":"2025-07-03T10:11:04.051517Z","shell.execute_reply":"2025-07-03T10:11:04.058805Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n       'Class'],\n      dtype='object')"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# The classes are heavily skewed we need to solve this issue later.\nprint('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:11:06.315730Z","iopub.execute_input":"2025-07-03T10:11:06.316070Z","iopub.status.idle":"2025-07-03T10:11:06.332114Z","shell.execute_reply.started":"2025-07-03T10:11:06.316038Z","shell.execute_reply":"2025-07-03T10:11:06.330988Z"}},"outputs":[{"name":"stdout","text":"No Frauds 99.83 % of the dataset\nFrauds 0.17 % of the dataset\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\n\n# RobustScaler is less prone to outliers.\n\nstd_scaler = StandardScaler()\nrob_scaler = RobustScaler()\n\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\ndf.drop(['Time','Amount'], axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:11:09.014303Z","iopub.execute_input":"2025-07-03T10:11:09.014708Z","iopub.status.idle":"2025-07-03T10:11:09.086285Z","shell.execute_reply.started":"2025-07-03T10:11:09.014678Z","shell.execute_reply":"2025-07-03T10:11:09.085283Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"scaled_amount = df['scaled_amount']\nscaled_time = df['scaled_time']\n\ndf.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndf.insert(0, 'scaled_amount', scaled_amount)\ndf.insert(1, 'scaled_time', scaled_time)\n\n# Amount and Time are Scaled!\n\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:11:11.948003Z","iopub.execute_input":"2025-07-03T10:11:11.948776Z","iopub.status.idle":"2025-07-03T10:11:12.007353Z","shell.execute_reply.started":"2025-07-03T10:11:11.948743Z","shell.execute_reply":"2025-07-03T10:11:12.006317Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"   scaled_amount  scaled_time        V1        V2        V3        V4  \\\n0       1.783274    -0.994983 -1.359807 -0.072781  2.536347  1.378155   \n1      -0.269825    -0.994983  1.191857  0.266151  0.166480  0.448154   \n2       4.983721    -0.994972 -1.358354 -1.340163  1.773209  0.379780   \n3       1.418291    -0.994972 -0.966272 -0.185226  1.792993 -0.863291   \n4       0.670579    -0.994960 -1.158233  0.877737  1.548718  0.403034   \n\n         V5        V6        V7        V8  ...       V20       V21       V22  \\\n0 -0.338321  0.462388  0.239599  0.098698  ...  0.251412 -0.018307  0.277838   \n1  0.060018 -0.082361 -0.078803  0.085102  ... -0.069083 -0.225775 -0.638672   \n2 -0.503198  1.800499  0.791461  0.247676  ...  0.524980  0.247998  0.771679   \n3 -0.010309  1.247203  0.237609  0.377436  ... -0.208038 -0.108300  0.005274   \n4 -0.407193  0.095921  0.592941 -0.270533  ...  0.408542 -0.009431  0.798278   \n\n        V23       V24       V25       V26       V27       V28  Class  \n0 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053      0  \n1  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724      0  \n2  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0  \n3 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458      0  \n4 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153      0  \n\n[5 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>scaled_amount</th>\n      <th>scaled_time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>...</th>\n      <th>V20</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.783274</td>\n      <td>-0.994983</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>...</td>\n      <td>0.251412</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.269825</td>\n      <td>-0.994983</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>...</td>\n      <td>-0.069083</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.983721</td>\n      <td>-0.994972</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>...</td>\n      <td>0.524980</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.418291</td>\n      <td>-0.994972</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>...</td>\n      <td>-0.208038</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.670579</td>\n      <td>-0.994960</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>...</td>\n      <td>0.408542</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 31 columns</p>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"fraud_df = df.loc[df['Class'] == 1]\nnon_fraud_df = df.loc[df['Class'] == 0]\n#non_fraud_df = non_fraud_df.loc[non_fraud_df.sample(n=100*len(fraud_df)).index]\ndata = pd.concat([fraud_df, non_fraud_df])\n\nX = data.drop('Class', axis=1)\ny = data['Class']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:11:14.669058Z","iopub.execute_input":"2025-07-03T10:11:14.669377Z","iopub.status.idle":"2025-07-03T10:11:14.789311Z","shell.execute_reply.started":"2025-07-03T10:11:14.669354Z","shell.execute_reply":"2025-07-03T10:11:14.788383Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\n\nclass_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y), y=y)\nprint(class_weights)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:11:18.012290Z","iopub.execute_input":"2025-07-03T10:11:18.013242Z","iopub.status.idle":"2025-07-03T10:11:18.068376Z","shell.execute_reply.started":"2025-07-03T10:11:18.013210Z","shell.execute_reply":"2025-07-03T10:11:18.067469Z"}},"outputs":[{"name":"stdout","text":"[  0.50086524 289.43800813]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Our data is already scaled we should split our training and test sets\nfrom sklearn.model_selection import train_test_split\n\n# This is explicitly used for undersampling.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:11:21.246125Z","iopub.execute_input":"2025-07-03T10:11:21.246443Z","iopub.status.idle":"2025-07-03T10:11:21.459541Z","shell.execute_reply.started":"2025-07-03T10:11:21.246420Z","shell.execute_reply":"2025-07-03T10:11:21.458477Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Turn the values into an array for feeding the classification algorithms.\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:11:22.949433Z","iopub.execute_input":"2025-07-03T10:11:22.950115Z","iopub.status.idle":"2025-07-03T10:11:22.991718Z","shell.execute_reply.started":"2025-07-03T10:11:22.950088Z","shell.execute_reply":"2025-07-03T10:11:22.990822Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import SGDClassifier, RidgeClassifier\nfrom keras.models import Model, Sequential\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nclassifiers = {\n    \"Logistic Regression\": LogisticRegression(solver='liblinear', random_state=42, class_weight='balanced'),\n    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n    \"Ridge Classifier\": RidgeClassifier(random_state=42, class_weight='balanced'),\n    \"SGD Classifier\": SGDClassifier(loss='log_loss', max_iter=1000, random_state=42, class_weight='balanced'),\n    \"SVM\": SVC(kernel=\"rbf\", C=10, probability=True, random_state=42, class_weight='balanced'),\n    \"XGBoost\": XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric=\"logloss\", random_state=42, class_weight='balanced'),\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:11:25.254549Z","iopub.execute_input":"2025-07-03T10:11:25.254986Z","iopub.status.idle":"2025-07-03T10:11:25.571673Z","shell.execute_reply.started":"2025-07-03T10:11:25.254954Z","shell.execute_reply":"2025-07-03T10:11:25.570793Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n    score = accuracy_score(y_test, y_pred)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"has a score of\", round(score.mean(), 2) * 100, \"% accuracy score\")\n    print(\"Classification Report: \\n\", classification_report(y_test, y_pred))\n    print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred), \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:11:32.463885Z","iopub.execute_input":"2025-07-03T10:11:32.464205Z","iopub.status.idle":"2025-07-03T10:25:23.830208Z","shell.execute_reply.started":"2025-07-03T10:11:32.464182Z","shell.execute_reply":"2025-07-03T10:25:23.829138Z"}},"outputs":[{"name":"stdout","text":"Classifiers:  LogisticRegression has a score of 98.0 % accuracy score\nClassification Report: \n               precision    recall  f1-score   support\n\n           0       1.00      0.98      0.99     56864\n           1       0.06      0.92      0.11        98\n\n    accuracy                           0.98     56962\n   macro avg       0.53      0.95      0.55     56962\nweighted avg       1.00      0.98      0.99     56962\n\nConfusion Matrix: \n [[55478  1386]\n [    8    90]] \n\nClassifiers:  RandomForestClassifier has a score of 100.0 % accuracy score\nClassification Report: \n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     56864\n           1       0.96      0.74      0.84        98\n\n    accuracy                           1.00     56962\n   macro avg       0.98      0.87      0.92     56962\nweighted avg       1.00      1.00      1.00     56962\n\nConfusion Matrix: \n [[56861     3]\n [   25    73]] \n\nClassifiers:  RidgeClassifier has a score of 99.0 % accuracy score\nClassification Report: \n               precision    recall  f1-score   support\n\n           0       1.00      0.99      0.99     56864\n           1       0.10      0.85      0.17        98\n\n    accuracy                           0.99     56962\n   macro avg       0.55      0.92      0.58     56962\nweighted avg       1.00      0.99      0.99     56962\n\nConfusion Matrix: \n [[56076   788]\n [   15    83]] \n\nClassifiers:  SGDClassifier has a score of 93.0 % accuracy score\nClassification Report: \n               precision    recall  f1-score   support\n\n           0       1.00      0.93      0.96     56864\n           1       0.02      0.93      0.04        98\n\n    accuracy                           0.93     56962\n   macro avg       0.51      0.93      0.50     56962\nweighted avg       1.00      0.93      0.96     56962\n\nConfusion Matrix: \n [[53003  3861]\n [    7    91]] \n\nClassifiers:  SVC has a score of 100.0 % accuracy score\nClassification Report: \n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     56864\n           1       0.48      0.76      0.58        98\n\n    accuracy                           1.00     56962\n   macro avg       0.74      0.88      0.79     56962\nweighted avg       1.00      1.00      1.00     56962\n\nConfusion Matrix: \n [[56783    81]\n [   24    74]] \n\nClassifiers:  XGBClassifier has a score of 100.0 % accuracy score\nClassification Report: \n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     56864\n           1       0.91      0.81      0.85        98\n\n    accuracy                           1.00     56962\n   macro avg       0.95      0.90      0.93     56962\nweighted avg       1.00      1.00      1.00     56962\n\nConfusion Matrix: \n [[56856     8]\n [   19    79]] \n\n","output_type":"stream"}],"execution_count":15}]}